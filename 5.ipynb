{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570dbb57",
   "metadata": {},
   "source": [
    "# Day 5\n",
    "## Pytorch\n",
    "* Build an MLP in Pytorch\n",
    "* Train MNIST/Fashion-MNIST (CPU)\n",
    "* Add weight decay, run with SGD vs Adam, add LR scheduler. \n",
    "\n",
    "### Check: \n",
    "* MNIST: test accuracy > 97% \n",
    "* MLP < 10 epochs (Adam helps)\n",
    "\n",
    "### Interview drill \n",
    "Differences between weight decay and L2 in Adam (decoupled vs classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3869adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "896fcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute mean / std of the dataset and use it for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc18583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEVICE] Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ---- Configuration ----\n",
    "DATASET = datasets.FashionMNIST\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "LR = 2e-3\n",
    "WD = 1e-4\n",
    "H = 42\n",
    "COSINE_LR = True # Use cosine learning rate schedule\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"[DEVICE] Using device: {DEVICE}\")\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "    torch.backends.mps.deterministic = True\n",
    "    torch.backends.mps.benchmark = False\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8a811",
   "metadata": {},
   "source": [
    "## Fashion MNIST data preprocessing\n",
    "\n",
    "**Fashion MNIST** stores pixel values as unsigned 8-bit integers in the range 0...255. Dividing by 255 converts this to floats in $[0,1]$. Necessary before mean/std normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7f0e5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Mean: 0.28604060411453247, std: 0.3530242443084717\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_train = datasets.FashionMNIST(root=\"./data\", train=True,  download=True)\n",
    "data = raw_train.data.float().div_(255)\n",
    "mean = data.mean().item()\n",
    "std = data.std().item()\n",
    "\n",
    "print(f\"[Data] Mean: {mean}, std: {std}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b9be4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Train size: 60000\n",
      "[Data] Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# --- Reload datasets with transforms (standard) ---\n",
    "train_ds = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"[Data] Train size:\", len(train_ds))\n",
    "print(\"[Data] Test size:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3388e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3e70f",
   "metadata": {},
   "source": [
    "## MLP Implementation\n",
    "\n",
    "*view* - reshapes tensors without copying memory when possible. Pass the new shape, -1: infer dimension automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32d0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dimension=28*28,\n",
    "                 hidden_dimensions=(256,128),\n",
    "                 output_dimension=10,\n",
    "                 droupout_probability=0.1,\n",
    "                 use_batchnorm=False):\n",
    "        \"\"\"Initialize a simple MLP with one hidden layer. Initializes weights and biases for the two layers at random.\n",
    "\n",
    "        Args:\n",
    "            input_dimension (int): Dimensionality of input data (D).\n",
    "            hidden_dimensions (tuple of int): Dimensionality of hidden layers.\n",
    "            output_dimension (int): Number of classes (C).\n",
    "            droupout_probability (float): Dropout probability, between 0 and 1.\n",
    "            use_batchnorm (bool): Whether to use batch normalization after each hidden layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        last_dimension = input_dimension\n",
    "        for hidden_dimension in hidden_dimensions:\n",
    "            layers.append(nn.Linear(last_dimension, hidden_dimension))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dimension))\n",
    "            layers.append(nn.ReLU())\n",
    "            if droupout_probability > 0:\n",
    "                layers.append(nn.Dropout(droupout_probability))\n",
    "            last_dimension = hidden_dimension\n",
    "\n",
    "        layers.append(nn.Linear(last_dimension, output_dimension))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[0]\n",
    "        X = X.view(N, -1) # reshape (N, 28, 28) -> (N, 28*28)\n",
    "        return self.net(X)\n",
    "\n",
    "model = MLP(droupout_probability=0.1,\n",
    "            use_batchnorm=False).to(DEVICE)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59b81518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# ---- Optimizer/Loss ---- \n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS) if COSINE_LR else None\n",
    "\n",
    "# ---- Train/eval ----\n",
    "def train_epoch(model:nn.Module,\n",
    "                loader:DataLoader,\n",
    "                optimizer:optim.Optimizer,\n",
    "                loss:nn.Module) -> tuple:\n",
    "    \"\"\"Trains one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train.\n",
    "        loader (DataLoader): DataLoader for training data.\n",
    "        optimizers (torch.optim.Optimizer): Optimizer to use.\n",
    "        loss (callable): Loss function.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy)\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "    for (x, y) in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        batch_loss = loss(logits, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item() * y.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8f1099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model:nn.Module,\n",
    "             loader:DataLoader,\n",
    "             loss:nn.Module) -> tuple:\n",
    "    model.eval()\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss_value = loss(logits, y) # Note: loss takes logits/targets, not probabilities\n",
    "        total_loss += loss_value.item() * y.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c5ed0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vojtahavlicek/onramp/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Epoch  1/10 | Train loss: 0.5146, accuracy: 0.8151 | Test loss: 0.4252, accuracy: 0.8441 | Learning rate: 0.001951\n",
      "[Model] New best model saved with accuracy: 0.8441\n",
      "[Training] Epoch  2/10 | Train loss: 0.3681, accuracy: 0.8651 | Test loss: 0.3719, accuracy: 0.8652 | Learning rate: 0.001809\n",
      "[Model] New best model saved with accuracy: 0.8652\n",
      "[Training] Epoch  3/10 | Train loss: 0.3269, accuracy: 0.8801 | Test loss: 0.3754, accuracy: 0.8661 | Learning rate: 0.001588\n",
      "[Model] New best model saved with accuracy: 0.8661\n",
      "[Training] Epoch  4/10 | Train loss: 0.3010, accuracy: 0.8879 | Test loss: 0.3516, accuracy: 0.8726 | Learning rate: 0.001309\n",
      "[Model] New best model saved with accuracy: 0.8726\n",
      "[Training] Epoch  5/10 | Train loss: 0.2803, accuracy: 0.8954 | Test loss: 0.3373, accuracy: 0.8773 | Learning rate: 0.001000\n",
      "[Model] New best model saved with accuracy: 0.8773\n",
      "[Training] Epoch  6/10 | Train loss: 0.2569, accuracy: 0.9047 | Test loss: 0.3233, accuracy: 0.8841 | Learning rate: 0.000691\n",
      "[Model] New best model saved with accuracy: 0.8841\n",
      "[Training] Epoch  7/10 | Train loss: 0.2369, accuracy: 0.9115 | Test loss: 0.3214, accuracy: 0.8834 | Learning rate: 0.000412\n",
      "[Training] Epoch  8/10 | Train loss: 0.2202, accuracy: 0.9169 | Test loss: 0.3062, accuracy: 0.8912 | Learning rate: 0.000191\n",
      "[Model] New best model saved with accuracy: 0.8912\n",
      "[Training] Epoch  9/10 | Train loss: 0.2067, accuracy: 0.9230 | Test loss: 0.3045, accuracy: 0.8913 | Learning rate: 0.000049\n",
      "[Model] New best model saved with accuracy: 0.8913\n",
      "[Training] Epoch 10/10 | Train loss: 0.1986, accuracy: 0.9265 | Test loss: 0.3005, accuracy: 0.8928 | Learning rate: 0.000000\n",
      "[Model] New best model saved with accuracy: 0.8928\n",
      "[Model] Best accuracy: 0.8928\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop ---\n",
    "best_accuracy = 0.0\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, loss)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, loss)\n",
    "\n",
    "    if scheduler: scheduler.step()\n",
    "\n",
    "    print(f\"[Training] Epoch {epoch:2d}/{epochs} | \"\n",
    "          f\"Train loss: {train_loss:.4f}, accuracy: {train_accuracy:.4f} | \"\n",
    "          f\"Test loss: {test_loss:.4f}, accuracy: {test_accuracy:.4f} | \"\n",
    "          f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), \"fashion_mnist_mlp_best.pth\")\n",
    "        print(f\"[Model] New best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(f\"[Model] Best accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273bff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
