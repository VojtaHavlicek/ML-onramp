{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2360df",
   "metadata": {},
   "source": [
    "# Deep Learning On Ramp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed868f",
   "metadata": {},
   "source": [
    "## Forward pass + loss (numpy only)\n",
    "**Goals** \n",
    "* Build intuition by coding an MLP without frameworks. \n",
    "* Lock in tensor shapes and loss definitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c1e144",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "1. Implement a 2-layer MLP (D -> H -> C) with ReLU and softmax (NumPy)\n",
    "2. Write cross_entropy and softmax logits (stable version: shift by max)\n",
    "3. Ad L2 regularization to the loss. \n",
    "\n",
    "**Checks**\n",
    "* Unit test: feed a tiny batch with hand-computed outputs\n",
    "* Verify that loss decreases with a single manual gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from onramp import ReLU, softmax\n",
    "\n",
    "# 2 Layer MLP\n",
    "D, H, C = 10, 20, 30 # Dimensions\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, \n",
    "                 D, \n",
    "                 H, \n",
    "                 C, \n",
    "                 W1=None, \n",
    "                 b1=None, \n",
    "                 W2=None, \n",
    "                 b2=None):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.C = C\n",
    "\n",
    "        # Conditional initialization of weights and biases\n",
    "        if W1 is None:\n",
    "            self.W1 = np.random.randn(D, H) * 0.01\n",
    "        else:\n",
    "            self.W1 = W1\n",
    "\n",
    "        if b1 is None:\n",
    "            self.b1 = np.zeros((1, H))\n",
    "        else:\n",
    "            self.b1 = b1\n",
    "\n",
    "        if W2 is None:\n",
    "            self.W2 = np.random.randn(H, C) * 0.01\n",
    "        else:\n",
    "            self.W2 = W2\n",
    "\n",
    "        if b2 is None:\n",
    "            self.b2 = np.zeros((1, C))\n",
    "        else:\n",
    "            self.b2 = b2\n",
    "        \n",
    "        # Forward pass function\n",
    "    def forward(self, X):\n",
    "        y1 = self.W1 @ X + self.b1 # Input -> Hidden\n",
    "        y1 = ReLU(y1) # Activation function\n",
    "        y2 = self.W2 @ y1 + self.b2 # Hidden -> Output\n",
    "        y2 = softmax(y2) # Activation function\n",
    "        return y2\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8d67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
